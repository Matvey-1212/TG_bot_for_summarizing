{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b205c8-6efc-461e-ac18-f60bed181f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_cache_dir = \"/home/maantonov_1/HF_data\"\n",
    "\n",
    "model_name = \"IlyaGusev/rugpt3medium_sum_gazeta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, cache_dir=custom_cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=custom_cache_dir).to(device)\n",
    "dataset = load_dataset(\"RussianNLP/Mixed-Summarization-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99ac01-6327-48dd-b02b-0f49777191ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    sep_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.convert_tokens_to_ids(\"\\n\")\n",
    "    for text, summary in zip(examples[\"text\"], examples[\"summary\"]):\n",
    "        article_tokens = tokenizer(text, max_length=600, truncation=True, add_special_tokens=False)[\"input_ids\"]\n",
    "        summary_tokens = tokenizer(summary, max_length=128, truncation=True, add_special_tokens=False)[\"input_ids\"]\n",
    "        input_ids = article_tokens + [sep_token_id] + summary_tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = [-100] * (len(article_tokens) + 1) + summary_tokens\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "    return {\"input_ids\": input_ids_list, \"attention_mask\": attention_mask_list, \"labels\": labels_list}\n",
    "\n",
    "try:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    dataset_path = \"DL_PROJECT/data3\"\n",
    "\n",
    "    tokenized_dataset = load_from_disk(dataset_path)\n",
    "except:\n",
    "    tokenized_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.save_to_disk(\"DL_PROJECT/data3\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    input_ids_batch, attention_mask_batch, labels_batch = [], [], []\n",
    "    for item in batch:\n",
    "        pad_len = max_length - len(item[\"input_ids\"])\n",
    "        input_ids_batch.append(item[\"input_ids\"] + [tokenizer.pad_token_id] * pad_len)\n",
    "        attention_mask_batch.append(item[\"attention_mask\"] + [0] * pad_len)\n",
    "        labels_batch.append(item[\"labels\"] + [-100] * pad_len)\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids_batch, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask_batch, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels_batch, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=8, num_workers=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610a2ce-5d8a-4479-820d-1476e6c9748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler('cuda')\n",
    "accumulation_steps = 8\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    running_loss = 0\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "        progress_bar.set_postfix({\"Loss\": running_loss / (i + 1), \"cur loss\":loss.item() * accumulation_steps})\n",
    "    progress_bar.close()\n",
    "    save_path = f\"./gpt3_sum_epoch_{epoch + 1}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e2303-df6c-47e9-878b-e6de65028aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-retinanet]",
   "language": "python",
   "name": "conda-env-.conda-retinanet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
